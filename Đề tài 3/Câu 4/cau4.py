from pyspark import SparkContext
from pyspark.streaming import StreamingContext
import csv
from io import StringIO

# T·∫°o SparkContext v√† StreamingContext m·ªói 5 gi√¢y x·ª≠ l√Ω 1 batch
sc = SparkContext("local[2]", "BoltonStatsStreaming")
ssc = StreamingContext(sc, 5)

def parse_line(line):
    return next(csv.reader(StringIO(line)))

def process_rdd(rdd):
    if not rdd.isEmpty():
        parsed = rdd.map(parse_line)
        
        # L·ªçc c√°c tr·∫≠n c√≥ ƒë·ªôi Bolton
        bolton_matches = parsed.filter(lambda row: "Bolton" in (row[2], row[3]))
    
        if not bolton_matches.isEmpty():
            # T·ªïng s·ªë b√†n Bolton ghi ƒë∆∞·ª£c
            bolton_goals = bolton_matches.map(lambda row: 
                int(row[4]) if row[2] == "Bolton" else int(row[5])
            )


            total_goals = bolton_goals.reduce(lambda x, y: x + y)
            match_count = bolton_matches.count()
            avg_goals = round(total_goals / float(match_count), 2)
            
            print(f"üî• Bolton: {match_count} tr·∫≠n | T·ªïng b√†n: {total_goals} | Hi·ªáu su·∫•t: {avg_goals}")
        else:
            print("‚ÑπÔ∏è Kh√¥ng c√≥ tr·∫≠n Bolton trong batch n√†y.")
    else:
        print("‚ö†Ô∏è Batch r·ªóng!")

# ƒê·ªçc d·ªØ li·ªáu stream t·ª´ th∆∞ m·ª•c ƒë·∫ßu v√†o
lines = ssc.textFileStream("/home/quzkhanh/Code/BigData/stream_input")
lines.foreachRDD(process_rdd)

# ssc.start()
# ssc.awaitTermination()

ssc.start()
ssc.awaitTerminationOrTimeout(60)  # D·ª´ng sau 60s
ssc.stop(stopSparkContext=True, stopGraceFully=True)
print("‚úÖ ƒê√£ x·ª≠ l√Ω xong t·∫•t c·∫£ file v√† d·ª´ng stream")

